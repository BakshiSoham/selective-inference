
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Selection &#8212; Selection Documentation</title>
    <link rel="stylesheet" href="../../_static/selection.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.0.1.dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />


  </head>
  <body>
<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
 <a href="../../index.html">
  <img src="../../_static/logo.png" height="200" width="200" alt="Selection logo"  border="0" /><h1>Post-selection inference</h1>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
  <li><a href="../../index.html">Selection home</a> |&nbsp;</li>
 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">

  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">algorithms.lasso</a><ul>
<li><a class="reference internal" href="#module-algorithms-lasso">Module: <code class="docutils literal"><span class="pre">algorithms.lasso</span></code></a></li>
<li><a class="reference internal" href="#classes">Classes</a><ul>
<li><a class="reference internal" href="#rosi"><code class="docutils literal"><span class="pre">ROSI</span></code></a></li>
<li><a class="reference internal" href="#rosi-modelq"><code class="docutils literal"><span class="pre">ROSI_modelQ</span></code></a></li>
<li><a class="reference internal" href="#data-carving"><code class="docutils literal"><span class="pre">data_carving</span></code></a></li>
<li><a class="reference internal" href="#data-splitting"><code class="docutils literal"><span class="pre">data_splitting</span></code></a></li>
<li><a class="reference internal" href="#lasso"><code class="docutils literal"><span class="pre">lasso</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#functions">Functions</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/api/generated/selection.algorithms.lasso.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>

<div id="searchbox-site" style="display: none">
  <h3>Search this site</h3>
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" size="13" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    </p>
</div>
<script type="text/javascript">$('#searchbox-ml').show(0);</script>
<script type="text/javascript">$('#searchbox-site').show(0);</script>


        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="algorithms-lasso">
<h1>algorithms.lasso<a class="headerlink" href="#algorithms-lasso" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-algorithms-lasso">
<h2>Module: <code class="xref py py-mod docutils literal"><span class="pre">algorithms.lasso</span></code><a class="headerlink" href="#module-algorithms-lasso" title="Permalink to this headline">¶</a></h2>
<p>Inheritance diagram for <code class="docutils literal"><span class="pre">selection.algorithms.lasso</span></code>:</p>
<span class="target" id="module-selection.algorithms.lasso"></span><p>This module contains a class <a class="reference internal" href="#lasso">lasso</a> that implements
post selection for the lasso
as described in <a href="#id31"><span class="problematic" id="id32">`post selection LASSO`_</span></a>.
.. _covTest: <a class="reference external" href="http://arxiv.org/abs/1301.7161">http://arxiv.org/abs/1301.7161</a>
.. _Kac Rice: <a class="reference external" href="http://arxiv.org/abs/1308.3020">http://arxiv.org/abs/1308.3020</a>
.. _Spacings: <a class="reference external" href="http://arxiv.org/abs/1401.3889">http://arxiv.org/abs/1401.3889</a>
.. _post selection LASSO: <a class="reference external" href="http://arxiv.org/abs/1311.6238">http://arxiv.org/abs/1311.6238</a>
.. _sample carving: <a class="reference external" href="http://arxiv.org/abs/1410.2597">http://arxiv.org/abs/1410.2597</a></p>
</div>
<div class="section" id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="rosi">
<h3><a class="reference internal" href="#selection.algorithms.lasso.ROSI" title="selection.algorithms.lasso.ROSI"><code class="xref py py-class docutils literal"><span class="pre">ROSI</span></code></a><a class="headerlink" href="#rosi" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="selection.algorithms.lasso.ROSI">
<em class="property">class </em><code class="descclassname">selection.algorithms.lasso.</code><code class="descname">ROSI</code><span class="sig-paren">(</span><em>loglike</em>, <em>feature_weights</em>, <em>approximate_inverse='BN'</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#selection.algorithms.lasso.lasso" title="selection.algorithms.lasso.lasso"><code class="xref py py-class docutils literal"><span class="pre">selection.algorithms.lasso.lasso</span></code></a></p>
<p>A class for the LASSO for post-selection inference.
The problem solved is
.. math:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>\<span class="n">text</span><span class="p">{</span><span class="n">minimize</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">beta</span><span class="p">}</span> \<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="n">n</span><span class="p">}</span> \<span class="o">|</span><span class="n">y</span><span class="o">-</span><span class="n">X</span>\<span class="n">beta</span>\<span class="o">|^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span>
    \<span class="k">lambda</span> \<span class="o">|</span>\<span class="n">beta</span>\<span class="o">|</span><span class="n">_1</span>
</pre></div>
</div>
<p>where <span class="math">\(\lambda\)</span> is <cite>lam</cite>.
Notes
—–
In solving the debiasing problem to approximate the inverse
of (X^TWX) in a GLM, this class makes the implicit assumption
that the scaling of X is such that diag(X^TWX) is O(n)
with n=X.shape[0]. That is, X’s are similar to IID samples
from a population that does not depend on n.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>loglike</em>, <em>feature_weights</em>, <em>approximate_inverse='BN'</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new post-selection for the LASSO problem
Parameters
———-
loglike : <cite>regreg.smooth.glm.glm</cite></p>
<blockquote>
<div>A (negative) log-likelihood as implemented in <cite>regreg</cite>.</div></blockquote>
<dl class="docutils">
<dt>feature_weights <span class="classifier-delimiter">:</span> <span class="classifier">np.ndarray</span></dt>
<dd>Feature weights for L-1 penalty. If a float,
it is brodcast to all features.</dd>
<dt>approximate_inverse <span class="classifier-delimiter">:</span> <span class="classifier">str (optional)</span></dt>
<dd>One of “JM” (Javanmard, Montanari) or “BN” (Boot, Niedderling) or None.
A form of approximate inverse when p is close to (or larger) than n.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.ROSI.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>lasso_solution=None</em>, <em>solve_args={'tol': 1e-12</em>, <em>'min_its': 50}</em>, <em>debiasing_args={}</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the lasso using <cite>regreg</cite>.
This sets the attributes <cite>soln</cite>, <cite>onestep</cite> and
forms the constraints necessary for post-selection inference
by calling <cite>form_constraints()</cite>.
Parameters
———-
lasso_solution : optional</p>
<blockquote>
<div>If not None, this is taken to be the solution
of the optimization problem. No checks
are done, though the implied affine
constraints will generally not be satisfied.</div></blockquote>
<dl class="docutils">
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">keyword args</span></dt>
<dd>Passed to <cite>regreg.problems.simple_problem.solve</cite>.</dd>
<dt>debiasing_args <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Arguments passed to <cite>.debiased_lasso.debiasing_matrix</cite>
or <cite>.debiased_lasso.pseudoinverse_debiasing_matrix</cite> depending
on <cite>self.approximate_inverse</cite>.</dd>
</dl>
<dl class="docutils">
<dt>soln <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Solution to lasso.</dd>
</dl>
<p class="rubric">Notes</p>
<p>If <cite>self</cite> already has an attribute <cite>lasso_solution</cite>
this will be taken to be the solution and
no optimization problem will be solved. Supplying
the optional argument <cite>lasso_solution</cite> will
overwrite <cite>self</cite>’s <cite>lasso_solution</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.ROSI.summary">
<code class="descname">summary</code><span class="sig-paren">(</span><em>level=0.95</em>, <em>compute_intervals=False</em>, <em>dispersion=None</em>, <em>truth=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Summary table for inference adjusted for selection.
Parameters
———-
level : float</p>
<blockquote>
<div>Form level*100% selective confidence intervals.</div></blockquote>
<dl class="docutils">
<dt>compute_intervals <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Should we compute confidence intervals?</dd>
<dt>dispersion <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimate of dispersion. Defaults to a Pearson’s X^2 estimate in the relaxed model.</dd>
<dt>truth <span class="classifier-delimiter">:</span> <span class="classifier">np.array</span></dt>
<dd>True values of each beta for selected variables. If not None, a column ‘pval’ are p-values
computed under these corresponding null hypotheses.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pval_summary</strong> : np.recarray</p>
<blockquote class="last">
<div><p>Array with one entry per active variable.
Columns are ‘variable’, ‘pval’, ‘lasso’, ‘onestep’, ‘lower_trunc’, ‘upper_trunc’, ‘sd’.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.ROSI.soln">
<code class="descname">soln</code><a class="headerlink" href="#selection.algorithms.lasso.ROSI.soln" title="Permalink to this definition">¶</a></dt>
<dd><p>Solution to the lasso problem, set by <cite>fit</cite> method.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="selection.algorithms.lasso.ROSI.gaussian">
<em class="property">static </em><code class="descname">gaussian</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>feature_weights</em>, <em>sigma=1.0</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em>, <em>approximate_inverse=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.gaussian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Squared-error LASSO with feature weights.
Objective function is
$$
beta mapsto frac{1}{2} |Y-Xbeta|^2_2 + sum_{i=1}^p lambda_i <a href="#id1"><span class="problematic" id="id2">|\beta_i|</span></a>
$$
where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>sigma <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>Noise variance. Set to 1 if <cite>covariance_estimator</cite> is not None.
This scales the loglikelihood by <cite>sigma**(-2)</cite>.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">callable (optional)</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of some of the
rows and columns of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="staticmethod">
<dt id="selection.algorithms.lasso.ROSI.logistic">
<em class="property">static </em><code class="descname">logistic</code><span class="sig-paren">(</span><em>X</em>, <em>successes</em>, <em>feature_weights</em>, <em>trials=None</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em>, <em>approximate_inverse=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.logistic" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Logistic LASSO with feature weights.
Objective function is
$$
beta mapsto ell(Xbeta) + sum_{i=1}^p lambda_i <a href="#id3"><span class="problematic" id="id4">|\beta_i|</span></a>
$$
where <span class="math">\(\ell\)</span> is the negative of the logistic
log-likelihood (half the logistic deviance)
and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>successes <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – response vector. An integer number of successes.
For data that is proportions, multiply the proportions
by the number of trials first.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>trials <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (optional)</span></dt>
<dd>Number of trials per response, defaults to
ones the same shape as Y.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="staticmethod">
<dt id="selection.algorithms.lasso.ROSI.poisson">
<em class="property">static </em><code class="descname">poisson</code><span class="sig-paren">(</span><em>X</em>, <em>counts</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em>, <em>approximate_inverse=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.poisson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Poisson log-linear LASSO with feature weights.
Objective function is
$$
beta mapsto ell^{text{Poisson}}(beta) + sum_{i=1}^p lambda_i <a href="#id5"><span class="problematic" id="id6">|\beta_i|</span></a>
$$
where <span class="math">\(\ell^{\text{Poisson}}\)</span> is the negative
of the log of the Poisson likelihood (half the deviance)
and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>counts <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="attribute">
<dt id="selection.algorithms.lasso.ROSI.constraints">
<code class="descname">constraints</code><a class="headerlink" href="#selection.algorithms.lasso.ROSI.constraints" title="Permalink to this definition">¶</a></dt>
<dd><p>Affine constraints for this LASSO problem.
These are the constraints determined only
by the active block.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.ROSI.coxph">
<code class="descname">coxph</code><span class="sig-paren">(</span><em>X</em>, <em>times</em>, <em>status</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.coxph" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Cox proportional hazards LASSO with feature weights.
Objective function is
$$
beta mapsto ell^{text{Cox}}(beta) + sum_{i=1}^p lambda_i <a href="#id7"><span class="problematic" id="id8">|\beta_i|</span></a>
$$
where <span class="math">\(\ell^{\text{Cox}}\)</span> is the
negative of the log of the Cox partial
likelihood and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Uses Efron’s tie breaking method.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>times <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the survival times.</dd>
<dt>status <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the censoring status.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI.sqrt_lasso">
<code class="descname">sqrt_lasso</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>feature_weights</em>, <em>quadratic=None</em>, <em>covariance='parametric'</em>, <em>sigma_estimate='truncated'</em>, <em>solve_args={'min_its': 200}</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI.sqrt_lasso" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Use sqrt-LASSO to choose variables.
Objective function is
$$
beta mapsto |Y-Xbeta|_2 + sum_{i=1}^p lambda_i <a href="#id9"><span class="problematic" id="id10">|\beta_i|</span></a>
$$
where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>. After solving the problem
treat as if <cite>gaussian</cite> with implied variance and choice of
multiplier. See arxiv.org/abs/1504.08031 for details.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
<dt>covariance <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>One of ‘parametric’ or ‘sandwich’. Method
used to estimate covariance for inference
in second stage.</dd>
<dt>sigma_estimate <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>One of ‘truncated’ or ‘OLS’. Method
used to estimate <span class="math">\(\sigma\)</span> when using
parametric covariance.</dd>
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Arguments passed to solver.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>Unlike other variants of LASSO, this
solves the problem on construction as the active
set is needed to find equivalent gaussian LASSO.
Assumes parametric model is correct for inference,
i.e. does not accept a covariance estimator.</p>
</dd></dl>

</div>
<div class="section" id="rosi-modelq">
<h3><a class="reference internal" href="#selection.algorithms.lasso.ROSI_modelQ" title="selection.algorithms.lasso.ROSI_modelQ"><code class="xref py py-class docutils literal"><span class="pre">ROSI_modelQ</span></code></a><a class="headerlink" href="#rosi-modelq" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="selection.algorithms.lasso.ROSI_modelQ">
<em class="property">class </em><code class="descclassname">selection.algorithms.lasso.</code><code class="descname">ROSI_modelQ</code><span class="sig-paren">(</span><em>Q</em>, <em>X</em>, <em>y</em>, <em>feature_weights</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#selection.algorithms.lasso.lasso" title="selection.algorithms.lasso.lasso"><code class="xref py py-class docutils literal"><span class="pre">selection.algorithms.lasso.lasso</span></code></a></p>
<p>A class for the LASSO for post-selection inference
in which
The problem solved is
.. math:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>\<span class="n">text</span><span class="p">{</span><span class="n">minimize</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">beta</span><span class="p">}</span> <span class="o">-</span><span class="p">(</span><span class="n">X</span>\<span class="n">beta</span><span class="p">)</span><span class="o">^</span><span class="n">Ty</span> <span class="o">+</span> \<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="p">}</span> \<span class="n">beta</span><span class="o">^</span><span class="n">TQ</span>\<span class="n">beta</span> <span class="o">+</span>
    \<span class="n">sum_i</span> \<span class="n">lambda_i</span> <span class="o">|</span>\<span class="n">beta_i</span><span class="o">|</span>
</pre></div>
</div>
<p>where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Notes
—–
In solving the debiasing problem to approximate the inverse
of (X^TWX) in a GLM, this class makes the implicit assumption
that the scaling of X is such that diag(X^TWX) is O(n)
with n=X.shape[0]. That is, X’s are similar to IID samples
from a population that does not depend on n.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>Q</em>, <em>X</em>, <em>y</em>, <em>feature_weights</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new post-selection for the LASSO problem
Parameters
———-
Q : np.ndarray((p,p))
X : np.ndarray((n, p))
y : np.ndarray(n)
feature_weights : np.ndarray</p>
<blockquote>
<div>Feature weights for L-1 penalty. If a float,
it is brodcast to all features.</div></blockquote>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.ROSI_modelQ.constraints">
<code class="descname">constraints</code><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.constraints" title="Permalink to this definition">¶</a></dt>
<dd><p>Affine constraints for this LASSO problem.
These are the constraints determined only
by the active block.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.coxph">
<code class="descname">coxph</code><span class="sig-paren">(</span><em>X</em>, <em>times</em>, <em>status</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.coxph" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Cox proportional hazards LASSO with feature weights.
Objective function is
$$
beta mapsto ell^{text{Cox}}(beta) + sum_{i=1}^p lambda_i <a href="#id11"><span class="problematic" id="id12">|\beta_i|</span></a>
$$
where <span class="math">\(\ell^{\text{Cox}}\)</span> is the
negative of the log of the Cox partial
likelihood and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Uses Efron’s tie breaking method.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>times <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the survival times.</dd>
<dt>status <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the censoring status.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.gaussian">
<code class="descname">gaussian</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>feature_weights</em>, <em>sigma=1.0</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.gaussian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Squared-error LASSO with feature weights.
Objective function is
$$
beta mapsto frac{1}{2} |Y-Xbeta|^2_2 + sum_{i=1}^p lambda_i <a href="#id13"><span class="problematic" id="id14">|\beta_i|</span></a>
$$
where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>sigma <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>Noise variance. Set to 1 if <cite>covariance_estimator</cite> is not None.
This scales the loglikelihood by <cite>sigma**(-2)</cite>.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">callable (optional)</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of some of the
rows and columns of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.logistic">
<code class="descname">logistic</code><span class="sig-paren">(</span><em>X</em>, <em>successes</em>, <em>feature_weights</em>, <em>trials=None</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.logistic" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Logistic LASSO with feature weights.
Objective function is
$$
beta mapsto ell(Xbeta) + sum_{i=1}^p lambda_i <a href="#id15"><span class="problematic" id="id16">|\beta_i|</span></a>
$$
where <span class="math">\(\ell\)</span> is the negative of the logistic
log-likelihood (half the logistic deviance)
and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>successes <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – response vector. An integer number of successes.
For data that is proportions, multiply the proportions
by the number of trials first.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>trials <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (optional)</span></dt>
<dd>Number of trials per response, defaults to
ones the same shape as Y.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.poisson">
<code class="descname">poisson</code><span class="sig-paren">(</span><em>X</em>, <em>counts</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.poisson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Poisson log-linear LASSO with feature weights.
Objective function is
$$
beta mapsto ell^{text{Poisson}}(beta) + sum_{i=1}^p lambda_i <a href="#id17"><span class="problematic" id="id18">|\beta_i|</span></a>
$$
where <span class="math">\(\ell^{\text{Poisson}}\)</span> is the negative
of the log of the Poisson likelihood (half the deviance)
and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>counts <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="attribute">
<dt id="selection.algorithms.lasso.ROSI_modelQ.soln">
<code class="descname">soln</code><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.soln" title="Permalink to this definition">¶</a></dt>
<dd><p>Solution to the lasso problem, set by <cite>fit</cite> method.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.sqrt_lasso">
<code class="descname">sqrt_lasso</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>feature_weights</em>, <em>quadratic=None</em>, <em>covariance='parametric'</em>, <em>sigma_estimate='truncated'</em>, <em>solve_args={'min_its': 200}</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.sqrt_lasso" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Use sqrt-LASSO to choose variables.
Objective function is
$$
beta mapsto |Y-Xbeta|_2 + sum_{i=1}^p lambda_i <a href="#id19"><span class="problematic" id="id20">|\beta_i|</span></a>
$$
where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>. After solving the problem
treat as if <cite>gaussian</cite> with implied variance and choice of
multiplier. See arxiv.org/abs/1504.08031 for details.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
<dt>covariance <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>One of ‘parametric’ or ‘sandwich’. Method
used to estimate covariance for inference
in second stage.</dd>
<dt>sigma_estimate <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>One of ‘truncated’ or ‘OLS’. Method
used to estimate <span class="math">\(\sigma\)</span> when using
parametric covariance.</dd>
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Arguments passed to solver.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>Unlike other variants of LASSO, this
solves the problem on construction as the active
set is needed to find equivalent gaussian LASSO.
Assumes parametric model is correct for inference,
i.e. does not accept a covariance estimator.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>solve_args={'tol': 1e-12</em>, <em>'min_its': 50}</em>, <em>debiasing_args={}</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the lasso using <cite>regreg</cite>.
This sets the attributes <cite>soln</cite>, <cite>onestep</cite> and
forms the constraints necessary for post-selection inference
by calling <cite>form_constraints()</cite>.
Parameters
———-
lasso_solution : optional</p>
<blockquote>
<div>If not None, this is taken to be the solution
of the optimization problem. No checks
are done, though the implied affine
constraints will generally not be satisfied.</div></blockquote>
<dl class="docutils">
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">keyword args</span></dt>
<dd>Passed to <cite>regreg.problems.simple_problem.solve</cite>.</dd>
</dl>
<dl class="docutils">
<dt>soln <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Solution to lasso.</dd>
</dl>
<p class="rubric">Notes</p>
<p>If <cite>self</cite> already has an attribute <cite>lasso_solution</cite>
this will be taken to be the solution and
no optimization problem will be solved. Supplying
the optional argument <cite>lasso_solution</cite> will
overwrite <cite>self</cite>’s <cite>lasso_solution</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.ROSI_modelQ.summary">
<code class="descname">summary</code><span class="sig-paren">(</span><em>level=0.05</em>, <em>compute_intervals=False</em>, <em>dispersion=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.ROSI_modelQ.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Summary table for inference adjusted for selection.
Parameters
———-
level : float</p>
<blockquote>
<div>Form level*100% selective confidence intervals.</div></blockquote>
<dl class="docutils">
<dt>compute_intervals <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Should we compute confidence intervals?</dd>
<dt>dispersion <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Estimate of dispersion. Defaults to a Pearson’s X^2 estimate in the relaxed model.</dd>
</dl>
<dl class="docutils">
<dt>pval_summary <span class="classifier-delimiter">:</span> <span class="classifier">np.recarray</span></dt>
<dd>Array with one entry per active variable.
Columns are ‘variable’, ‘pval’, ‘lasso’, ‘onestep’, ‘lower_trunc’, ‘upper_trunc’, ‘sd’.</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="data-carving">
<h3><a class="reference internal" href="#selection.algorithms.lasso.data_carving" title="selection.algorithms.lasso.data_carving"><code class="xref py py-class docutils literal"><span class="pre">data_carving</span></code></a><a class="headerlink" href="#data-carving" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="selection.algorithms.lasso.data_carving">
<em class="property">class </em><code class="descclassname">selection.algorithms.lasso.</code><code class="descname">data_carving</code><span class="sig-paren">(</span><em>loglike_select</em>, <em>loglike_inference</em>, <em>loglike_full</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_carving" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#selection.algorithms.lasso.lasso" title="selection.algorithms.lasso.lasso"><code class="xref py py-class docutils literal"><span class="pre">selection.algorithms.lasso.lasso</span></code></a></p>
<p class="rubric">Notes</p>
<p>Even if a covariance estimator is supplied,
we assume that we can drop inactive constraints,
i.e. the same (asymptotic) independence that
holds for parametric model is assumed to hold here
as well.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.data_carving.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>loglike_select</em>, <em>loglike_inference</em>, <em>loglike_full</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_carving.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.data_carving.constraints">
<code class="descname">constraints</code><a class="headerlink" href="#selection.algorithms.lasso.data_carving.constraints" title="Permalink to this definition">¶</a></dt>
<dd><p>Affine constraints for this LASSO problem.
These are the constraints determined only
by the active block.</p>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.data_carving.soln">
<code class="descname">soln</code><a class="headerlink" href="#selection.algorithms.lasso.data_carving.soln" title="Permalink to this definition">¶</a></dt>
<dd><p>Solution to the lasso problem, set by <cite>fit</cite> method.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.data_carving.summary">
<code class="descname">summary</code><span class="sig-paren">(</span><em>alternative='twosided'</em>, <em>level=0.95</em>, <em>compute_intervals=False</em>, <em>truth=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_carving.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Summary table for inference adjusted for selection.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>alternative</strong> : str</p>
<blockquote>
<div><p>One of [“twosided”,”onesided”]</p>
</div></blockquote>
<p><strong>level</strong> : float</p>
<blockquote>
<div><p>Form level*100% selective confidence intervals.</p>
</div></blockquote>
<p><strong>compute_intervals</strong> : bool</p>
<blockquote>
<div><p>Should we compute confidence intervals?</p>
</div></blockquote>
<p><strong>truth</strong> : np.array</p>
<blockquote>
<div><p>True values of each beta for selected variables. If not None, a column ‘pval’ are p-values
computed under these corresponding null hypotheses.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pval_summary</strong> : np.recarray</p>
<blockquote class="last">
<div><p>Array with one entry per active variable.
Columns are ‘variable’, ‘pval’, ‘lasso’, ‘onestep’, ‘lower_trunc’, ‘upper_trunc’, ‘sd’.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="data-splitting">
<h3><a class="reference internal" href="#selection.algorithms.lasso.data_splitting" title="selection.algorithms.lasso.data_splitting"><code class="xref py py-class docutils literal"><span class="pre">data_splitting</span></code></a><a class="headerlink" href="#data-splitting" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="selection.algorithms.lasso.data_splitting">
<em class="property">class </em><code class="descclassname">selection.algorithms.lasso.</code><code class="descname">data_splitting</code><span class="sig-paren">(</span><em>loglike_select</em>, <em>loglike_inference</em>, <em>loglike_full</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_splitting" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#selection.algorithms.lasso.data_carving" title="selection.algorithms.lasso.data_carving"><code class="xref py py-class docutils literal"><span class="pre">selection.algorithms.lasso.data_carving</span></code></a></p>
<dl class="method">
<dt id="selection.algorithms.lasso.data_splitting.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>loglike_select</em>, <em>loglike_inference</em>, <em>loglike_full</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_splitting.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.data_splitting.hypothesis_test">
<code class="descname">hypothesis_test</code><span class="sig-paren">(</span><em>variable</em>, <em>df=inf</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_splitting.hypothesis_test" title="Permalink to this definition">¶</a></dt>
<dd><p>Wald test for an active variable.</p>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.data_splitting.constraints">
<code class="descname">constraints</code><a class="headerlink" href="#selection.algorithms.lasso.data_splitting.constraints" title="Permalink to this definition">¶</a></dt>
<dd><p>Affine constraints for this LASSO problem.
These are the constraints determined only
by the active block.</p>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.data_splitting.soln">
<code class="descname">soln</code><a class="headerlink" href="#selection.algorithms.lasso.data_splitting.soln" title="Permalink to this definition">¶</a></dt>
<dd><p>Solution to the lasso problem, set by <cite>fit</cite> method.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.data_splitting.summary">
<code class="descname">summary</code><span class="sig-paren">(</span><em>alternative='twosided'</em>, <em>level=0.95</em>, <em>compute_intervals=False</em>, <em>truth=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.data_splitting.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Summary table for inference adjusted for selection.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>alternative</strong> : str</p>
<blockquote>
<div><p>One of [“twosided”,”onesided”]</p>
</div></blockquote>
<p><strong>level</strong> : float</p>
<blockquote>
<div><p>Form level*100% selective confidence intervals.</p>
</div></blockquote>
<p><strong>compute_intervals</strong> : bool</p>
<blockquote>
<div><p>Should we compute confidence intervals?</p>
</div></blockquote>
<p><strong>truth</strong> : np.array</p>
<blockquote>
<div><p>True values of each beta for selected variables. If not None, a column ‘pval’ are p-values
computed under these corresponding null hypotheses.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pval_summary</strong> : np.recarray</p>
<blockquote class="last">
<div><p>Array with one entry per active variable.
Columns are ‘variable’, ‘pval’, ‘lasso’, ‘onestep’, ‘lower_trunc’, ‘upper_trunc’, ‘sd’.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="lasso">
<h3><a class="reference internal" href="#selection.algorithms.lasso.lasso" title="selection.algorithms.lasso.lasso"><code class="xref py py-class docutils literal"><span class="pre">lasso</span></code></a><a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="selection.algorithms.lasso.lasso">
<em class="property">class </em><code class="descclassname">selection.algorithms.lasso.</code><code class="descname">lasso</code><span class="sig-paren">(</span><em>loglike</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>ignore_inactive_constraints=False</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>A class for the LASSO for post-selection inference.
The problem solved is
.. math:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>\<span class="n">text</span><span class="p">{</span><span class="n">minimize</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">beta</span><span class="p">}</span> \<span class="n">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="n">n</span><span class="p">}</span> \<span class="o">|</span><span class="n">y</span><span class="o">-</span><span class="n">X</span>\<span class="n">beta</span>\<span class="o">|^</span><span class="mi">2</span><span class="n">_2</span> <span class="o">+</span>
    \<span class="k">lambda</span> \<span class="o">|</span>\<span class="n">beta</span>\<span class="o">|</span><span class="n">_1</span>
</pre></div>
</div>
<p>where <span class="math">\(\lambda\)</span> is <cite>lam</cite>.</p>
<dl class="method">
<dt id="selection.algorithms.lasso.lasso.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>loglike</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>ignore_inactive_constraints=False</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new post-selection dor the LASSO problem
Parameters
———-
loglike : <cite>regreg.smooth.glm.glm</cite></p>
<blockquote>
<div>A (negative) log-likelihood as implemented in <cite>regreg</cite>.</div></blockquote>
<dl class="docutils">
<dt>feature_weights <span class="classifier-delimiter">:</span> <span class="classifier">np.ndarray</span></dt>
<dd>Feature weights for L-1 penalty. If a float,
it is brodcast to all features.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">callable (optional)</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
</dl>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.lasso.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>lasso_solution=None</em>, <em>solve_args={'tol': 1e-12</em>, <em>'min_its': 50}</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the lasso using <cite>regreg</cite>.
This sets the attributes <cite>soln</cite>, <cite>onestep</cite> and
forms the constraints necessary for post-selection inference
by calling <cite>form_constraints()</cite>.
Parameters
———-
lasso_solution : optional</p>
<blockquote>
<div>If not None, this is taken to be the solution
of the optimization problem. No checks
are done, though the implied affine
constraints will generally not be satisfied.</div></blockquote>
<dl class="docutils">
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">keyword args</span></dt>
<dd>Passed to <cite>regreg.problems.simple_problem.solve</cite>.</dd>
</dl>
<dl class="docutils">
<dt>soln <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Solution to lasso.</dd>
</dl>
<p class="rubric">Notes</p>
<p>If <cite>self</cite> already has an attribute <cite>lasso_solution</cite>
this will be taken to be the solution and
no optimization problem will be solved. Supplying
the optional argument <cite>lasso_solution</cite> will
overwrite <cite>self</cite>’s <cite>lasso_solution</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="selection.algorithms.lasso.lasso.summary">
<code class="descname">summary</code><span class="sig-paren">(</span><em>alternative='twosided'</em>, <em>level=0.95</em>, <em>compute_intervals=False</em>, <em>truth=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Summary table for inference adjusted for selection.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>alternative</strong> : str</p>
<blockquote>
<div><p>One of [“twosided”,”onesided”]</p>
</div></blockquote>
<p><strong>level</strong> : float</p>
<blockquote>
<div><p>Form level*100% selective confidence intervals.</p>
</div></blockquote>
<p><strong>compute_intervals</strong> : bool</p>
<blockquote>
<div><p>Should we compute confidence intervals?</p>
</div></blockquote>
<p><strong>truth</strong> : np.array</p>
<blockquote>
<div><p>True values of each beta for selected variables. If not None, a column ‘pval’ are p-values
computed under these corresponding null hypotheses.</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pval_summary</strong> : np.recarray</p>
<blockquote class="last">
<div><p>Array with one entry per active variable.
Columns are ‘variable’, ‘pval’, ‘lasso’, ‘onestep’, ‘lower_trunc’, ‘upper_trunc’, ‘sd’.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.lasso.soln">
<code class="descname">soln</code><a class="headerlink" href="#selection.algorithms.lasso.lasso.soln" title="Permalink to this definition">¶</a></dt>
<dd><p>Solution to the lasso problem, set by <cite>fit</cite> method.</p>
</dd></dl>

<dl class="attribute">
<dt id="selection.algorithms.lasso.lasso.constraints">
<code class="descname">constraints</code><a class="headerlink" href="#selection.algorithms.lasso.lasso.constraints" title="Permalink to this definition">¶</a></dt>
<dd><p>Affine constraints for this LASSO problem.
These are the constraints determined only
by the active block.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="selection.algorithms.lasso.lasso.gaussian">
<em class="property">static </em><code class="descname">gaussian</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>feature_weights</em>, <em>sigma=1.0</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.gaussian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Squared-error LASSO with feature weights.
Objective function is
$$
beta mapsto frac{1}{2} |Y-Xbeta|^2_2 + sum_{i=1}^p lambda_i <a href="#id21"><span class="problematic" id="id22">|\beta_i|</span></a>
$$
where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>sigma <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>Noise variance. Set to 1 if <cite>covariance_estimator</cite> is not None.
This scales the loglikelihood by <cite>sigma**(-2)</cite>.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">callable (optional)</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of some of the
rows and columns of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="staticmethod">
<dt id="selection.algorithms.lasso.lasso.logistic">
<em class="property">static </em><code class="descname">logistic</code><span class="sig-paren">(</span><em>X</em>, <em>successes</em>, <em>feature_weights</em>, <em>trials=None</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.logistic" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Logistic LASSO with feature weights.
Objective function is
$$
beta mapsto ell(Xbeta) + sum_{i=1}^p lambda_i <a href="#id23"><span class="problematic" id="id24">|\beta_i|</span></a>
$$
where <span class="math">\(\ell\)</span> is the negative of the logistic
log-likelihood (half the logistic deviance)
and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>successes <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – response vector. An integer number of successes.
For data that is proportions, multiply the proportions
by the number of trials first.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>trials <span class="classifier-delimiter">:</span> <span class="classifier">ndarray (optional)</span></dt>
<dd>Number of trials per response, defaults to
ones the same shape as Y.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="staticmethod">
<dt id="selection.algorithms.lasso.lasso.coxph">
<em class="property">static </em><code class="descname">coxph</code><span class="sig-paren">(</span><em>X</em>, <em>times</em>, <em>status</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.coxph" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Cox proportional hazards LASSO with feature weights.
Objective function is
$$
beta mapsto ell^{text{Cox}}(beta) + sum_{i=1}^p lambda_i <a href="#id25"><span class="problematic" id="id26">|\beta_i|</span></a>
$$
where <span class="math">\(\ell^{\text{Cox}}\)</span> is the
negative of the log of the Cox partial
likelihood and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Uses Efron’s tie breaking method.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>times <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the survival times.</dd>
<dt>status <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the censoring status.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="staticmethod">
<dt id="selection.algorithms.lasso.lasso.poisson">
<em class="property">static </em><code class="descname">poisson</code><span class="sig-paren">(</span><em>X</em>, <em>counts</em>, <em>feature_weights</em>, <em>covariance_estimator=None</em>, <em>quadratic=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.poisson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Poisson log-linear LASSO with feature weights.
Objective function is
$$
beta mapsto ell^{text{Poisson}}(beta) + sum_{i=1}^p lambda_i <a href="#id27"><span class="problematic" id="id28">|\beta_i|</span></a>
$$
where <span class="math">\(\ell^{\text{Poisson}}\)</span> is the negative
of the log of the Poisson likelihood (half the deviance)
and <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>counts <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>covariance_estimator <span class="classifier-delimiter">:</span> <span class="classifier">optional</span></dt>
<dd>If None, use the parameteric
covariance estimate of the selected model.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>If not None, <cite>covariance_estimator</cite> should
take arguments (beta, active, inactive)
and return an estimate of the covariance of
<span class="math">\((\bar{\beta}_E, \nabla \ell(\bar{\beta}_E)_{-E})\)</span>,
the unpenalized estimator and the inactive
coordinates of the gradient of the likelihood at
the unpenalized estimator.</p>
<dl class="staticmethod">
<dt id="selection.algorithms.lasso.lasso.sqrt_lasso">
<em class="property">static </em><code class="descname">sqrt_lasso</code><span class="sig-paren">(</span><em>X</em>, <em>Y</em>, <em>feature_weights</em>, <em>quadratic=None</em>, <em>covariance='parametric'</em>, <em>sigma_estimate='truncated'</em>, <em>solve_args={'min_its': 200}</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.lasso.sqrt_lasso" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Use sqrt-LASSO to choose variables.
Objective function is
$$
beta mapsto |Y-Xbeta|_2 + sum_{i=1}^p lambda_i <a href="#id29"><span class="problematic" id="id30">|\beta_i|</span></a>
$$
where <span class="math">\(\lambda\)</span> is <cite>feature_weights</cite>. After solving the problem
treat as if <cite>gaussian</cite> with implied variance and choice of
multiplier. See arxiv.org/abs/1504.08031 for details.
Parameters
———-
X : ndarray</p>
<blockquote>
<div>Shape (n,p) – the design matrix.</div></blockquote>
<dl class="docutils">
<dt>Y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd>Shape (n,) – the response.</dd>
<dt>feature_weights: [float, sequence]</dt>
<dd>Penalty weights. An intercept, or other unpenalized
features are handled by setting those entries of
<cite>feature_weights</cite> to 0. If <cite>feature_weights</cite> is
a float, then all parameters are penalized equally.</dd>
<dt>quadratic <span class="classifier-delimiter">:</span> <span class="classifier"><cite>regreg.identity_quadratic.identity_quadratic</cite> (optional)</span></dt>
<dd>An optional quadratic term to be added to the objective.
Can also be a linear term by setting quadratic
coefficient to 0.</dd>
<dt>covariance <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>One of ‘parametric’ or ‘sandwich’. Method
used to estimate covariance for inference
in second stage.</dd>
<dt>sigma_estimate <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>One of ‘truncated’ or ‘OLS’. Method
used to estimate <span class="math">\(\sigma\)</span> when using
parametric covariance.</dd>
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>Arguments passed to solver.</dd>
</dl>
<p>L : <cite>selection.algorithms.lasso.lasso</cite></p>
<p class="rubric">Notes</p>
<p>Unlike other variants of LASSO, this
solves the problem on construction as the active
set is needed to find equivalent gaussian LASSO.
Assumes parametric model is correct for inference,
i.e. does not accept a covariance estimator.</p>
</dd></dl>

</div>
</div>
<div class="section" id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="selection.algorithms.lasso.additive_noise">
<code class="descclassname">selection.algorithms.lasso.</code><code class="descname">additive_noise</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sigma</em>, <em>lam_frac=1.0</em>, <em>perturb_frac=0.2</em>, <em>y_star=None</em>, <em>coverage=0.95</em>, <em>ndraw=8000</em>, <em>compute_intervals=True</em>, <em>burnin=2000</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.additive_noise" title="Permalink to this definition">¶</a></dt>
<dd><p>Additive noise LASSO.
Parameters
———-
y : np.float</p>
<blockquote>
<div>Response vector</div></blockquote>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Design matrix</dd>
<dt>sigma <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Noise variance</dd>
<dt>lam_frac <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>Multiplier for choice of <span class="math">\(\lambda\)</span>. Defaults to 2.</dd>
<dt>perturb_frac <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>How much noise to add? Noise added has variance
proportional to existing variance.</dd>
<dt>coverage <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Coverage for selective intervals. Defaults to 0.95.</dd>
<dt>ndraw <span class="classifier-delimiter">:</span> <span class="classifier">int (optional)</span></dt>
<dd>How many draws to keep from Gibbs hit-and-run sampler.
Defaults to 8000.</dd>
<dt>burnin <span class="classifier-delimiter">:</span> <span class="classifier">int (optional)</span></dt>
<dd>Defaults to 2000.</dd>
<dt>compute_intervals <span class="classifier-delimiter">:</span> <span class="classifier">bool (optional)</span></dt>
<dd>Compute selective intervals?</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>results</strong> : [(variable, pvalue, interval)</p>
<blockquote>
<div><p>Indices of active variables,
selected (twosided) pvalue and selective interval.
If splitting, then each entry also includes
a (split_pvalue, split_interval) using stage_two
for inference.</p>
</div></blockquote>
<p><strong>randomized_lasso</strong> : <cite>lasso</cite></p>
<blockquote class="last">
<div><p>Results of fitting LASSO to randomized data.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="selection.algorithms.lasso.glm_parametric_estimator">
<code class="descclassname">selection.algorithms.lasso.</code><code class="descname">glm_parametric_estimator</code><span class="sig-paren">(</span><em>loglike</em>, <em>dispersion=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.glm_parametric_estimator" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Parametric estimator of covariance of</p>
<div class="math">
\[(ar{eta}_E, X_{-E}^T(y-\]</div>
</div></blockquote>
<dl class="docutils">
<dt>abla ell(X_Ear{eta}_E))</dt>
<dd><p class="first">the OLS estimator of population regression
coefficients and inactive correlation with the
OLS residuals.
If <cite>sigma</cite> is None, it computes usual unbiased estimate
of variance in Gaussian model and plugs it in,
assuming parametric form is correct.
Returns
——-
estimator : callable</p>
<blockquote class="last">
<div>Takes arguments (beta, active, inactive)</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="selection.algorithms.lasso.glm_sandwich_estimator">
<code class="descclassname">selection.algorithms.lasso.</code><code class="descname">glm_sandwich_estimator</code><span class="sig-paren">(</span><em>loss</em>, <em>B=1000</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.glm_sandwich_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bootstrap estimator of covariance of</p>
<div class="math">
\[(ar{eta}_E, X_{-E}^T(y-X_Ear{eta}_E)\]</div>
<p>the OLS estimator of population regression
coefficients and inactive correlation with the
OLS residuals.
Returns
——-
estimator : callable</p>
<blockquote>
<div>Takes arguments (beta, active, inactive)</div></blockquote>
</dd></dl>

<dl class="function">
<dt id="selection.algorithms.lasso.nominal_intervals">
<code class="descclassname">selection.algorithms.lasso.</code><code class="descname">nominal_intervals</code><span class="sig-paren">(</span><em>lasso_obj</em>, <em>level=0.95</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.nominal_intervals" title="Permalink to this definition">¶</a></dt>
<dd><p>Intervals for OLS parameters of active variables
that have not been adjusted for selection.</p>
</dd></dl>

<dl class="function">
<dt id="selection.algorithms.lasso.split_model">
<code class="descclassname">selection.algorithms.lasso.</code><code class="descname">split_model</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sigma=1</em>, <em>lam_frac=1.0</em>, <em>split_frac=0.9</em>, <em>stage_one=None</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.split_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a LASSO with a default choice of Lagrange parameter
equal to <cite>lam_frac</cite> times <span class="math">\(\sigma \cdot E(|X^T\epsilon|)\)</span>
with <span class="math">\(\epsilon\)</span> IID N(0,1) on a proportion (<cite>split_frac</cite>) of
the data.
Parameters
———-
y : np.float</p>
<blockquote>
<div>Response vector</div></blockquote>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Design matrix</dd>
<dt>sigma <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Noise variance</dd>
<dt>lam_frac <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>Multiplier for choice of <span class="math">\(\lambda\)</span>. Defaults to 2.</dd>
<dt>split_frac <span class="classifier-delimiter">:</span> <span class="classifier">float (optional)</span></dt>
<dd>What proportion of the data to use in the first stage?
Defaults to 0.9.</dd>
<dt>stage_one <span class="classifier-delimiter">:</span> <span class="classifier">[np.array(np.int), None] (optional)</span></dt>
<dd>Index of data points to be used in  first stage.
If None, a randomly chosen set of entries is used based on
<cite>split_frac</cite>.</dd>
</dl>
<dl class="docutils">
<dt>first_stage <span class="classifier-delimiter">:</span> <span class="classifier"><cite>lasso</cite></span></dt>
<dd>Lasso object from stage one.</dd>
<dt>stage_one <span class="classifier-delimiter">:</span> <span class="classifier">np.array(int)</span></dt>
<dd>Indices used for stage one.</dd>
<dt>stage_two <span class="classifier-delimiter">:</span> <span class="classifier">np.array(int)</span></dt>
<dd>Indices used for stage two.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="selection.algorithms.lasso.standard_lasso">
<code class="descclassname">selection.algorithms.lasso.</code><code class="descname">standard_lasso</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sigma=1</em>, <em>lam_frac=1.0</em>, <em>**solve_args</em><span class="sig-paren">)</span><a class="headerlink" href="#selection.algorithms.lasso.standard_lasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a LASSO with a default choice of Lagrange parameter
equal to <cite>lam_frac</cite> times <span class="math">\(\sigma \cdot E(|X^T\epsilon|)\)</span>
with <span class="math">\(\epsilon\)</span> IID N(0,1).
Parameters
———-
y : np.float</p>
<blockquote>
<div>Response vector</div></blockquote>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Design matrix</dd>
<dt>sigma <span class="classifier-delimiter">:</span> <span class="classifier">np.float</span></dt>
<dd>Noise variance</dd>
<dt>lam_frac <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>Multiplier for choice of <span class="math">\(\lambda\)</span></dd>
<dt>solve_args <span class="classifier-delimiter">:</span> <span class="classifier">keyword args</span></dt>
<dd>Passed to <cite>regreg.problems.simple_problem.solve</cite>.</dd>
</dl>
<dl class="docutils">
<dt>lasso_selection <span class="classifier-delimiter">:</span> <span class="classifier"><cite>lasso</cite></span></dt>
<dd>Instance of <cite>lasso</cite> after fitting.</dd>
</dl>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
  <li><a href="../../index.html">Selection home</a> |&nbsp;</li>
 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright J. Taylor and others.
      Last updated on Apr 23, 2019.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.3.
    </div>
  </body>
</html>